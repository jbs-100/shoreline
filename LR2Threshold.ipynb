{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOgDaTy/ZzMf9PXBrksESde"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Step 01:** Import the necessary libraries"],"metadata":{"id":"riVQxlDhIMPl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LCu-uv65Hfcn"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from sklearn.model_selection import train_test_split, cross_validate\n","from sklearn.metrics import mean_squared_error, r2_score"]},{"cell_type":"markdown","source":["**Step 02:** Get the transect besed data from Google Drive"],"metadata":{"id":"UUMfhnyOIQBH"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive'), force_remount = True"],"metadata":{"id":"lm80uLfkIPej"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv ('/content/drive/My Drive/Transects.csv')"],"metadata":{"id":"WMWy63CGITe_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 03:** Calculate product of clay and sand percentage"],"metadata":{"id":"ddtyLqUOIcX3"}},{"cell_type":"code","source":["data['Smeans'] = (data['S0'] + data['S10'] + data['S30'] + data['S60'] + data['S100'] + data['S200']) / 6\n","data['Cmeans'] = (data['C0'] + data['C10'] + data['C30'] + data['C60'] + data['C100'] + data['C200']) / 6\n","data['Pmeans'] = (data['Cmeans']*data['Smeans'])"],"metadata":{"id":"pw0vRtxlIcjQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 04:** Categorize overall dataset based on LR2"],"metadata":{"id":"J_0zGGJmIlH4"}},{"cell_type":"code","source":["df = pd.DataFrame(data)\n","df1 = pd.DataFrame(data[data['LR2']>=0.1])\n","df2 = pd.DataFrame(data[data['LR2']>=0.2])\n","df3 = pd.DataFrame(data[data['LR2']>=0.3])\n","df4 = pd.DataFrame(data[data['LR2']>=0.4])\n","df5 = pd.DataFrame(data[data['LR2']>=0.5])\n","df6 = pd.DataFrame(data[data['LR2']>=0.6])\n","df7 = pd.DataFrame(data[data['LR2']>=0.7])\n","df8 = pd.DataFrame(data[data['LR2']>=0.8])\n","df9 = pd.DataFrame(data[data['LR2']>=0.9])"],"metadata":{"id":"N0Rgk00LIlQu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 05:** Divide each dataset into erosional and accretional data based on LRR"],"metadata":{"id":"G9dDaVHlI4sv"}},{"cell_type":"code","source":["dataE = df[df['LRR'] < 0]\n","dataD = df[df['LRR'] > 0]\n","dataE1 = df1[df1['LRR'] < 0]\n","dataD1 = df1[df1['LRR'] > 0]\n","dataE2 = df2[df2['LRR'] < 0]\n","dataD2 = df2[df2['LRR'] > 0]\n","dataE3 = df3[df3['LRR'] < 0]\n","dataD3 = df3[df3['LRR'] > 0]\n","dataE4 = df4[df4['LRR'] < 0]\n","dataD4 = df4[df4['LRR'] > 0]\n","dataE5 = df5[df5['LRR'] < 0]\n","dataD5 = df5[df5['LRR'] > 0]\n","dataE6 = df6[df6['LRR'] < 0]\n","dataD6 = df6[df6['LRR'] > 0]\n","dataE7 = df7[df7['LRR'] < 0]\n","dataD7 = df7[df7['LRR'] > 0]\n","dataE8 = df8[df8['LRR'] < 0]\n","dataD8 = df8[df8['LRR'] > 0]\n","dataE9 = df9[df9['LRR'] < 0]\n","dataD9 = df9[df9['LRR'] > 0]"],"metadata":{"id":"O55AVCYmI4D4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 06:** Make machine learning models"],"metadata":{"id":"pK55fN2rRRGk"}},{"cell_type":"code","source":["rf = RandomForestRegressor(random_state = 42)\n","gb = GradientBoostingRegressor(random_state = 42)"],"metadata":{"id":"vw7qsHLCRRPA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 07:** For all datasets (Random Forest)\n","*   Designate x and y varibales\n","*   Split both x and y into training and testing datasets\n","*   Nornalize the dataset\n","*   Apply cross-validation and calculate R2 score and RMSE\n","*   Train the model and predict output\n","*   Calculate R2 score and RMSE for testing datasets\n","*   Calculate feature importance score."],"metadata":{"id":"hAFdX4AaRZZ_"}},{"cell_type":"code","source":["X = data[['Orientation', 'Rmeans', 'Width', 'Dist_sea', 'Slope', 'Dist_SS']] # Change the data each time\n","y = data['LRR']                                                              # Change the data each time\n","X_train, X_test, y_train, y_rfT0_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","cv_results = cross_validate(rf, X_train, y_train, cv = 5, n_jobs = -1, verbose = 2,\n","                            scoring = {'r2': 'r2', 'mse':'neg_mean_squared_error'})\n","\n","r2V_Trf_LR20 = np.mean(cv_results['test_r2'])\n","rmseV_Trf_LR20 = np.sqrt(np.abs(np.mean(cv_results['test_mse'])))\n","print(f'R2 (Validation): {r2V_Trf_LR20}')\n","print(f'RMSE (Validation): {rmseV_Trf_LR20}')\n","\n","rfT0 = rf.fit (X_train, y_train)\n","y_rfT0 = rfT0.predict (X_test)\n","\n","r2T_Trf_LR20 = r2_score (y_rfT0_test, y_rfT0)\n","rmseT_Trf_LR20 = np.sqrt(mean_squared_error (y_rfT0_test, y_rfT0))\n","print ('R2 (Testing):', r2T_Trf_LR20)\n","print ('RMSE (Testing):', rmseT_Trf_LR20)\n","print ('Feature Importance:', rfT0.feature_importances_)"],"metadata":{"id":"C4-3elFPRZg2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 07:** For all datasets (Gradient Boosting)\n","*   Designate x and y varibales\n","*   Split both x and y into training and testing datasets\n","*   Nornalize the dataset\n","*   Apply cross-validation and calculate R2 score and RMSE\n","*   Train the model and predict output\n","*   Calculate R2 score and RMSE for testing datasets\n","*   Calculate feature importance score."],"metadata":{"id":"JhNPFa-rUwcW"}},{"cell_type":"code","source":["X = data[['Orientation', 'Rmeans', 'Width_Mod', 'Dist_sea', 'Slope', 'Dist_SS']] # Change the data each time\n","y = data['LRR']                                                                  # Change the data each time\n","X_train, X_test, y_train, y_gbT0_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","cv_results = cross_validate(gb, X_train, y_train, cv = 5, n_jobs = -1, verbose = 2, scoring = {'r2': 'r2', 'mse':'neg_mean_squared_error'})\n","\n","r2V_Tgb_LR20 = np.mean(cv_results['test_r2'])\n","rmseV_Tgb_LR20 = np.sqrt(np.abs(np.mean(cv_results['test_mse'])))\n","print(f'R2 (Validation): {r2V_Tgb_LR20}')\n","print(f'RMSE (Validation): {rmseV_Tgb_LR20}')\n","\n","gbT0 = gb.fit (X_train, y_train)\n","y_gbT0 = gbT0.predict (X_test)\n","\n","r2T_Tgb_LR20 = r2_score (y_gbT0_test, y_gbT0)\n","rmseT_Tgb_LR20 = np.sqrt(mean_squared_error (y_gbT0_test, y_gbT0))\n","print ('R2 (Testing):', r2T_Tgb_LR20)\n","print ('RMSE (Testing):', rmseT_Tgb_LR20)\n","print ('Feature Importance:', gbT0.feature_importances_)"],"metadata":{"id":"1oOHQPdYUzEy"},"execution_count":null,"outputs":[]}]}